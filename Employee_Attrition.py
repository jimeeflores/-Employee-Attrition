# -*- coding: utf-8 -*-
"""M6_Jimena_Flores.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YwomnjoScwI36HU4suzbe-ufsQQMDR9a

# Reto: Ingeniería de Características.

Jimena Flores
"""

#Importamos Librerias
import pandas as pd
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd "/content/drive/MyDrive/Data Science Diplomado /M6/RETO"

# Lectura de Datos "Test"
datos = pd.read_csv("Test.csv")
datos.head()

df = pd.read_csv("Test.csv")

# Lectura de Datos "Train"
datos = pd.read_csv("Train.csv")
datos.head()

df2 = pd.read_csv("Train.csv")

"""# ***Base de datos 1 (TEST) ***

Imputación de valores faltantes
"""

missing_values = df.isnull().sum()

# Mostramos columnas con valores faltantes
missing_columns = missing_values[missing_values > 0]
missing_columns

"""Para la base de datos Test existen valores faltantes en las siguientes columnas:

Age: 161 valores perdidos

*  Time_of_service: 52 valores faltantes
*   Pay_Scale: 3 valores faltantes
*   Work_Life_balance: 5 valores faltantes
*   VAR2: 217 valores faltantes
*   VAR4: 298 valores faltantes




"""

#Imputación de Valores Faltantes

df['Age'].fillna(df['Age'].median(), inplace=True)
df['Time_of_service'].fillna(df['Time_of_service'].median(), inplace=True)
df['Pay_Scale'].fillna(df['Pay_Scale'].median(), inplace=True)
df['Work_Life_balance'].fillna(df['Work_Life_balance'].median(), inplace=True)
df['VAR2'].fillna(df['VAR2'].median(), inplace=True)
df['VAR4'].fillna(df['VAR4'].median(), inplace=True)

# Verificamos que no hay valores faltantes restantes
missing_values_after_imputation = df.isnull().sum()
missing_values_after_imputation[missing_values_after_imputation > 0]

#Detección de Valores atípicos
df.describe()

# Para visualizar posibles valores atípicos en las variables utilizaremos un gráfico de caja (box plot)
# Seleccionaremos algunas de las variables numéricas
!pip install matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))

# Boxplot de las variables 'Age' y 'Time_of_service'
df[['Age', 'Time_of_service']].plot(kind='box', vert=False, figsize=(10, 6))
plt.title('Box plot: Edad vs Tiempo de servicio')
plt.xlabel('Valor')
plt.grid(True)
plt.show()

"""No se dectectaron valores atípicos analizando y graficando la base de datos por lo que no se genera código para dicha sección.

One Hot Encoding
"""

# Aplicamos la técnica de One Hot encoding a las variables categóricas
df_encoded = pd.get_dummies(df, columns=['Gender', 'Education_Level', 'Relationship_Status', 'Hometown',
                                         'Unit', 'Decision_skill_possess', 'Compensation_and_Benefits'])
# Mostramos el Data Frame resultante
df_encoded.head()

# Analizamos qué columnas fueron eliminadas

original_columns = set(df.columns)
encoded_columns = set(df_encoded.columns)
columns_removed = list(original_columns - encoded_columns)
columns_removed

"""Análisis de columnas eliminadas
* Gender: Eliminada y reemplazada por columnas binarias "Gender_F" y "Gender_M".
* Compensation_and_Benefits: Eliminada y reemplazada por columnas como "Compensation_and_Benefits_type0", "Compensation_and_Benefits_type1". Representando cada tipo de compensación.
* Hometown: Eliminada y reemplazada por columnas "Hometown_Clinton", "Hometown_Franklin", que representan cada ciudad de origen.
* Unit: Eliminada y reemplazada por columnas como "Unit_IT", "Unit_Sales", etc., que indican la unidad a la que pertenece un empleado.
* Decision_skill_possess: Reemplazada por columnas que indican el tipo de habilidad de decisión poseída, como "Decision_skill_possess_Analytical", "Decision_skill_possess_Directive", etc.
* Relationship_Status: Reemplazada por columnas binarias "Relationship_Status_Married" y "Relationship_Status_Single".
* Education_Level: Eliminada y reemplazada por columnas que representan cada nivel educativo "Education_Level_1", "Education_Level_2", etc.

Detección de Variables para Binning
"""

# Ya que reducimos columnas al momento de aplicar One Hot Encoding debemos ajustar la selección de variables numéricas
numerical_columns= df_encoded.select_dtypes(include=[np.number]).columns

# Revisamos variables potencias para binning.
potential_binning = df_encoded[numerical_columns].nunique().sort_values()

potential_binning

"""Las variables con un número pequeño de valores únicos podrían beneficiarse del binning. En este caso las variables:


*   Travel_Rate", "VAR4", "VAR6", "VAR3", "VAR2", "VAR1", "Work_Life_balance", "Post_Level", "VAR7", "Time_since_promotion", "VAR5", "Pay_Scale"


Tienen entre 3 y 10 valores únicos, por lo que podríamos aplicar binning

Reducción de dimensionalidad PCA
"""

#Normalización de datos
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_encoded.select_dtypes(include=[np.number]))

# Aplicamos PCA
pca = PCA(n_components=0.95)  # Aquí estamos reteniendo el 95% de varianza
pca_result = pca.fit_transform(df_scaled)

# Creamos un Data Frame para mostrar los resultados
df_pca = pd.DataFrame(data=pca_result, columns=[f'PC{i+1}' for i in range(pca_result.shape[1])])

# Mostramos la varianza de cada dato para asegurarnos de que la técnica de PCA fue aplicada correctamente (Debemos tener una varianza baja)
variance = pca.explained_variance_ratio_

df_pca.head(), variance

print(pca.explained_variance_ratio_)

#Revisaremos gráficamente como se observa la varianza para verificar que el set de datos sigue siendo confiable y la calidad de datos garantizadaa
import matplotlib.pyplot as plt # Importa el módulo matplotlib.pyplot

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(variance) + 1), pd.Series(variance).cumsum(), marker='o', linestyle='-', color='b')
plt.title('Varianza acumulada por componentes')
plt.xlabel('Numero de componentes')
plt.ylabel('Varianza acumulada')
plt.grid(True)
plt.show()

n_components_retained = pca.n_components_

n_components_retained

"""# Conclusiones: Primer Data Base (Test)

Los componentes mínimos (variables) que se necesitan para asegurar un 95% de varianza son 14. Si revisamos el # de componentes que resultaron del proceso de One Hot Encoding nos da un resultado también de 14, por lo que nuestro set de datos está listo para el paso de modelado sin ninguna pérdida significativa de información.

---

# ***Base de datos 2 (Train)***
"""

# Analízamos la data (Lectura de datos Train)
df2.head()

"""Imputación de valores faltantes"""

missing_values_train = df2.isnull().sum()

# Mostramos las columnas que tiene valores faltantes
missing_columns_train = missing_values_train[missing_values_train > 0]
missing_columns_train

"""En el conjunto de datos, se encuentran valores faltantes en las siguientes columnas:

* Age: 412 valores faltantes
* Time_of_service: 144 valores faltantes
* Pay_Scale: 9 valores faltantes
* Work_Life_balance: 11 valores faltantes
* VAR2: 577 valores faltantes
* VAR4: 656 valores faltantes

Para la imputación, aplicaremos técnicas adecuadas dependiendo del tipo de variable:

* Variables numéricas ("Age", "Time_of_service", "Pay_Scale", "Work_Life_balance", "VAR2", "VAR4"):
* Utilizaremos la mediana para variables como "Age" y "Time_of_service", ya que es nos ayudará a integrar algún valores atípico (si es que hay)
* Para "Pay_Scale" y "Work_Life_balance", también utilizaremos la mediana, dado que representan categorías numéricas discretas.
* Para "VAR2" y "VAR4", se utilizará la mediana para mantener la coherencia de los datos.
"""

# Imputación usando Mediana
df2['Age'].fillna(df2['Age'].median(), inplace=True)
df2['Time_of_service'].fillna(df2['Time_of_service'].median(), inplace=True)
df2['Pay_Scale'].fillna(df2['Pay_Scale'].median(), inplace=True)
df2['Work_Life_balance'].fillna(df2['Work_Life_balance'].median(), inplace=True)
df2['VAR2'].fillna(df2['VAR2'].median(), inplace=True)
df2['VAR4'].fillna(df2['VAR4'].median(), inplace=True)

# Verificamos que no hay valores faltantes restantes
missing_values2 = df2.isnull().sum()
missing_values2[missing_values2 > 0]

"""Detección de Valores Atípicos"""

#Usaremos la desviación estándar para calcular los valores atípicos.

# Crear un DataFrame para almacenar los resultados de los valores atípicos basados en la desviación estándar
numerical_columns_df2 = df2.select_dtypes(include=[np.number]).columns
atipicos_std_df2 = pd.DataFrame()

# Detectar valores atípicos utilizando la desviación estándar
for col in numerical_columns_df2:
    mean = df2[col].mean()
    std_dev = df2[col].std()

  # Definir los límites basados en 3 desviaciones estándar
    lower_bound = mean - 3 * std_dev
    upper_bound = mean + 3 * std_dev

  # Identificar valores atípicos
    atipicos_std_df2[col] = ((df2[col] < lower_bound) | (df2[col] > upper_bound))

# Mostrar los valores atípicos basados en la desviación estándar
atipicos_std_df2.head()

#Contamos los valores atipicos
atipicos_count_std_df2 = atipicos_std_df2.sum()
atipicos_count_std_df2[atipicos_count_std_df2 > 0]

# Mostramos los valores atipicos en attrition_rate
# Attrition_rate

from matplotlib import pyplot as plt
import seaborn as sns
atipicos_std_df2.groupby('Attrition_rate').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""Dado que "Attrition_rate" parece ser una variable continua que puede representar una tasa, vamos a aplicar una transformación logarítmica (log-transformation) para reducir la influencia de los valores atípicos."""

for col in numerical_columns_df2:
    if atipicos_count_std_df2[col] > 0:
        # Añadimos una pequeña constante para evitar log(0)
        df2[col] = np.log1p(df2[col])

# Recalcular los valores atípicos después de la transformación logarítmica
atipicos_count_std_df2 = pd.DataFrame()
for col in numerical_columns_df2:
    mean_log = df2[col].mean()
    std_dev_log = df2[col].std()

for col in numerical_columns_df2:
    mean_log = df2[col].mean()
    std_dev_log = df2[col].std()
    # Definir los límites basados en 3 desviaciones estándar después de la transformación logarítmica
    lower_bound_log = mean_log - 3 * std_dev_log
    upper_bound_log = mean_log + 3 * std_dev_log

    # Crear un DataFrame vacío para almacenar los resultados de los valores atípicos
    atipicos_std_log_df2 = pd.DataFrame()

    # Identificar valores atípicos después de la transformación
    atipicos_std_log_df2[col] = ((df2[col] < lower_bound_log) | (df2[col] > upper_bound_log))

# Contar el número de valores atípicos por columna después de la transformación logarítmica
atipicos_count_std_log_df2 = atipicos_std_log_df2.sum()
#Mostramos los valores atipicos por columna

# Corregir el nombre de la variable aquí también
atipicos_count_std_log_df2[atipicos_count_std_log_df2 > 0]

"""One Hot Encoding"""

# Aplicación de One Hot Encoding después de la transformación
df2_encoded = pd.get_dummies(df2, columns=['Gender', 'Education_Level', 'Relationship_Status',
                                           'Hometown', 'Unit', 'Decision_skill_possess',
                                           'Compensation_and_Benefits'])

#Mostramos las primeras filas despues del One Hot Encoding
df2_encoded.head()

"""Detección de variables para Binning"""

# Seleccionamos las columnas numeros despues de OHE
numerical_columns_after_encoding_train = df2_encoded.select_dtypes(include=[np.number]).columns

# Revisamos potenciales variables para binning

potential_binning_vars_after_encoding_train = df2_encoded[numerical_columns_after_encoding_train].nunique().sort_values()

potential_binning_vars_after_encoding_train

"""Algunas variables podrían beneficiarse de técnica de binning
* Variables con pocos valores únicos:
"Travel_Rate", "VAR4", "VAR5", "VAR7", "VAR3", "VAR2", "VAR1", "Work_Life_balance", "Post_Level", "Time_since_promotion", "VAR6": Estas variables tienen entre 3 y 5 valores únicos y podrían ser adecuadas para técnicas de binning, donde se agrupan en intervalos.

Reducción de dimensionalidad con Análisis Factorial (FA)

Después de la normalización de los datos transformados, se aplicó FA para reducir la dimensionalidad y calcular la varianza explicada por cada uno de los 10 factores seleccionados.
"""

# Ajuste para calcular la varianza explicada utilizando FA
# Volveremos a crear el modelo de Factor Analysis con un número mayor de componentes para evaluar mejor los resultados
fa = FactorAnalysis(n_components=10)  # Intentando con 10 componentes como ejemplo inicial
fa_result = fa.fit_transform(df2_scaled)

# Calcular la varianza explicada por cada componente factorial utilizando la matriz de carga de factores
factor_loading_variance = np.var(fa.components_, axis=1)
total_variance = np.sum(factor_loading_variance)
explained_variance_ratio = factor_loading_variance / total_variance

# Mostrar la varianza explicada por cada componente
explained_variance_ratio, len(explained_variance_ratio)

"""El Análisis Factorial (FA) ha retenido 10 componentes, y la varianza explicada por cada componente es la siguiente:

* Componente 1: 6.88%
* Componente 2: 6.69%
* Componente 3: 5.64%
* Componente 4: 5.03%
* Componente 5: 3.66%
* Componente 6: 2.96%
* Componente 7: 2.60%
* Componente 8: 1.95%
* Componente 9: 1.91%
* Componente 10: 1.64%

Conclusiones Data Base 2 (Train)

Los 10 factores seleccionados explican diferentes porcentajes de la varianza total de los datos. Ningún factor domina completamente, lo que sugiere que los datos pueden estar bien distribuidos.
"""